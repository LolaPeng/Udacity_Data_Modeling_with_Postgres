                                                Data Modeling with Postgres

Project description: 

Sparkify is a startup which collected data about songs and user activity on their new streaming app. The analytics team of Sparkify wants to understand what songs users are listening to. The data are currently in two JSON files: logs and songs. The analytics team hope that data engineering team can create a Postgres database which works best for their pupose. 





Database design: 

Based on the requests from analytics team and the two JSON files, I designed a Star scheme with one fact table and four dimension tables. Please find the details below:

Fact table: 

1. songplay table

 songplay_id serial, 
 start_time timestamp NOT NULL,
 user_id int NOT NULL , 
 level varchar NOT NULL, 
 song_id varchar, 
 artist_id varchar, 
 session_id int NOT NULL, 
 location varchar NOT NULL , 
 user_agent varchar NOT NULL,
 PRIMARY KEY (songplay_id)
 
songplay table is the fact table which consists of the facts of the business process of Sparkify.
It can be connected with all other 4 dimensions table to get useful infomration needed in the future analysis. 

 
Dimension tables:

1. users table:

user_id int, 
first_name varchar NOT NULL, 
last_name varchar NOT NULL,
gender varchar NOT NULL, 
level varchar NOT NULL,
PRIMARY KEY (user_id))

users table is about users of Sparkify. It has the criticle user information which can be used in the user analysis to portait the users.


2. songs table:

song_id VARCHAR(18), 
title varchar NOT NULL, 
artist_id varchar NOT NULL, 
year int NOT NULL,                         
duration int NOT NULL,
PRIMARY KEY (song_id)

songs table is about the songs on Sparkify platform. It is useful when analysis anything regarding songs


3. artists table:

artist_id varchar, 
name varchar NOT NULL, 
location varchar,                                
latitude float, 
longitude float,
PRIMARY KEY (artist_id)

artists table is a table about the artists. It is usful when we need any information about aritsts.

4. time table

start_time timestamp PRIMARY KEY, 
hour int NOT NULL, 
day int NOT NULL, 
week int NOT NULL,                              
month int NOT NULL, 
year int NOT NULL, 
weekday int NOT NULL

time table is a table about the start time. It records the time by hour, day, week, month and year to make it easier for analytics team to conduct analysis on time.





ETL Process: 

Song Dataset

It is a subset of real data from Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRABCEI12903CDCF1A.json


Log Dataset

It is the second dataset consists of log files in JSON format generated by event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified congifurations. It is partitioned by year and month.

Here are two examples of the data:
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

Firstly, I created the songs and artists dimensional tables. 
Then I extract data for songs table, and aritsts table and insert record into songs table and aritsts table.

After processed the song data, I processed the log data. 
Firstly, I performed ETL on the log_data create the time and users dimensional tables, as well as the songplays fact table.
Then, I extract data for time table, users table and songplay table and insert records into these three tables.





Project Repository files: 

data/log_data

This dataset contains a collection of JSON log files. 
These files are used to build our Fact table (songpaly) and to build the dimension tables for users and time.

data/song_data

This dataset contains a collection of Song JSON files. 
These files are used to build dimension tables for songs and artists.

sql_queries.py

A Python script that defines all the SQL queries used by this project.

test.ipynb

A Python Jupyter Notebook that was used to test that data was loaded properly.

create_tables.py

This Python script recreates the tables used to store the data we need

etl.ipynb

A Python Jupyter Notebook that was used to test the ETL process.

etl.py

This Python script reads in the Log and Song data files, processes and inserts data into the database.






How To Run the Project: 

1. Complete sql_queries.py file based on the project requests
2. Run create_tables.py to create tables using the queries prepared in the sql_queries.py
3. Test if the tables we need are created correctly by running test.ipynb
4. Complete etl.ipynb to test if data are inserted correctly into each table we created and if the ETL process is built correctly
5. If all steps above are finished correctly, run create_tables.py to reset the tables and run etl.py to build the ETL

